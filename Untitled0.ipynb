{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvKiI+kfpRCzFmy45vC+3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antony2Ogillo/deep-learning-practical/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group members\n",
        "\n",
        "\n",
        "1. Antony Ogillo - IN14/00057/21  \n",
        "2. Hillary Kakuko IN14/00019/20\n",
        "3. Vincent Odiwuor IN14/00053/21\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UMZG8rZOmxDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "hamlet = gutenberg.words('shakespeare-hamlet.txt')\n",
        "text = ''\n",
        "for word in hamlet:  # For each word\n",
        "    text += str(word).lower()  # Convert to lower case and add to string variable\n",
        "    text += ' '  # Add space\n",
        "print('Corpus length, Hamlet only:', len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKDEXDMT40uo",
        "outputId": "7413a46c-3097-4033-97b6-5d3cf06bc7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length, Hamlet only: 166765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(text)))\n",
        "print('Total characters:', len(characters))\n",
        "char_indices = dict((l, i) for i, l in enumerate(characters))\n",
        "indices_char = dict((i, l) for i, l in enumerate(characters))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U5b8J3gXs-q",
        "outputId": "ab3dc565-f519-4702-c5cb-70dbb361182d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Break text into Features and Labels\n",
        "training_sequences = []  # Empty list to collect each sequence\n",
        "next_chars = []  # Empty list to collect next character in sequence\n",
        "seq_len, stride = 35, 1  # Define length of each input sequence & stride to move before sampling next sequence\n",
        "\n",
        "for i in range(0, len(text) - seq_len, stride):  # Loop over text with window of 35 characters, moving 1 stride at a time\n",
        "    training_sequences.append(text[i: i + seq_len])  # Append sequences to training_sequences\n",
        "    next_chars.append(text[i + seq_len])  # Append following character in sequence to next_chars\n",
        "\n",
        "# Print out sequences and labels to verify\n",
        "print('Number of sequences:', len(training_sequences))\n",
        "print('First sequences:', training_sequences[:1])\n",
        "print('Next characters in sequence:', next_chars[:1])\n",
        "print('Second sequences:', training_sequences[1:2])\n",
        "print('Next characters in sequence:', next_chars[1:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uJNtfk5YiR6",
        "outputId": "4b1f6db4-da37-4087-fa47-d9814645ec52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 166730\n",
            "First sequences: ['[ the tragedie of hamlet by william']\n",
            "Next characters in sequence: [' ']\n",
            "Second sequences: [' the tragedie of hamlet by william ']\n",
            "Next characters in sequence: ['s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.src.optimizers import RMSprop\n",
        "from keras.src.models import Sequential\n",
        "from keras.src.layers import Dense, SimpleRNN\n",
        "\n",
        "# 1. Prepare sample text data (replace this with your own corpus)\n",
        "text = \"\"\"The quick brown fox jumps over the lazy dog.\"\"\"\n",
        "text = text.lower()  # Lowercase for consistency\n",
        "\n",
        "# 2. Create character-to-index mappings\n",
        "characters = sorted(list(set(text)))  # Unique sorted characters\n",
        "char_to_idx = {char: idx for idx, char in enumerate(characters)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(characters)}\n",
        "\n",
        "# 3. Preprocess text into input/output sequences\n",
        "seq_len = 20  # Length of input sequences\n",
        "step = 3      # Step to slide the window\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "for i in range(0, len(text) - seq_len, step):\n",
        "    inputs.append(text[i:i + seq_len])\n",
        "    outputs.append(text[i + seq_len])\n",
        "\n",
        "# 4. Vectorize the data (one-hot encoding)\n",
        "X = np.zeros((len(inputs), seq_len, len(characters)), dtype=np.bool_)\n",
        "y = np.zeros((len(inputs), len(characters)), dtype=np.bool_)\n",
        "\n",
        "for i, input_seq in enumerate(inputs):\n",
        "    for t, char in enumerate(input_seq):\n",
        "        X[i, t, char_to_idx[char]] = 1\n",
        "    y[i, char_to_idx[outputs[i]]] = 1\n",
        "\n",
        "# 5. Build and compile the model\n",
        "def build_simple_rnn_model(seq_len, num_characters):\n",
        "    model = Sequential([\n",
        "        SimpleRNN(128, input_shape=(seq_len, num_characters), return_sequences=True),\n",
        "        SimpleRNN(128),\n",
        "        Dense(num_characters, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))\n",
        "    return model\n",
        "\n",
        "model = build_simple_rnn_model(seq_len=seq_len, num_characters=len(characters))\n",
        "\n",
        "# 6. Train the model (for demonstration, use more epochs in practice)\n",
        "model.fit(X, y, batch_size=128, epochs=20)\n",
        "\n",
        "# 7. Generate text\n",
        "def generate_text(model, seed_text, char_to_idx, idx_to_char, seq_len, num_chars=100):\n",
        "    generated = seed_text\n",
        "    for _ in range(num_chars):\n",
        "        x = np.zeros((1, seq_len, len(char_to_idx)))\n",
        "        for t, char in enumerate(seed_text[-seq_len:]):  # Use last 'seq_len' chars\n",
        "            x[0, t, char_to_idx[char]] = 1\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        next_idx = np.argmax(preds)\n",
        "        next_char = idx_to_char[next_idx]\n",
        "        generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char  # Slide the window\n",
        "    return generated\n",
        "\n",
        "# Test generation\n",
        "print(generate_text(model, \"the quick brown \", char_to_idx, idx_to_char, seq_len=seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5VeuBtpZcAE",
        "outputId": "0ab7c748-15b3-444d-fc10-619f12919e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 3.2380\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.6187\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.4156\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 4.2585\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.6848\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 3.3820\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 3.1705\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 2.6713\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 2.1756\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.2055\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.1756\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.1893\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 2.2528\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 2.1038\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 2.0679\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.3193\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 2.6498\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 2.2954\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 3.0995\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.7087\n",
            "the quick brown jpjrjpopjropjoyrjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjpjp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "import pickle\n",
        "from keras.src.models import Sequential\n",
        "from keras.src.layers import Dense, Bidirectional, Dropout, SimpleRNN, GRU, BatchNormalization\n",
        "from keras.src.optimizers import RMSprop\n",
        "from keras.src.callbacks import LambdaCallback, ModelCheckpoint\n",
        "\n",
        "# Ensure `characters`, `char_indices`, and `indices_char` are based on the correct text.\n",
        "# This is the text you're using for training:\n",
        "# Replace this with your actual training text variable (e.g., hamlet_text)\n",
        "training_text = ''\n",
        "for word in hamlet:  # For each word\n",
        "    training_text += str(word).lower()  # Convert to lower case and add to string variable\n",
        "    training_text += ' '  # Add space\n",
        "\n",
        "\n",
        "# Recreate characters, char_indices, and indices_char based on the correct training text\n",
        "characters = sorted(list(set(training_text)))\n",
        "char_indices = dict((l, i) for i, l in enumerate(characters))\n",
        "indices_char = dict((i, l) for i, l in enumerate(characters))\n",
        "\n",
        "\n",
        "x = np.zeros((len(training_sequences), 35, len(characters)), dtype=bool)\n",
        "y = np.zeros((len(training_sequences), len(characters)), dtype=bool)\n",
        "\n",
        "for index, sequence in enumerate(training_sequences):  # Iterate over training sequences\n",
        "    # Ensure sub_index stays within the bounds of seq_len (35)\n",
        "    for sub_index, chars in enumerate(sequence[:35]):  # Iterate over characters per sequence up to seq_len\n",
        "        # Check if the character is in char_indices before accessing it\n",
        "        if chars in char_indices:\n",
        "            x[index, sub_index, char_indices[chars]] = 1  # Update character position in feature matrix to 1\n",
        "        else:\n",
        "            # Handle the case where the character is not in the vocabulary\n",
        "            # You can either skip it, replace it with a special token, or retrain with a larger vocabulary\n",
        "            print(f\"Warning: Character '{chars}' not found in vocabulary. Skipping.\")\n",
        "    y[index, char_indices[next_chars[index]]] = 1  # Update character position in label matrix to 1\n",
        "\n",
        "# Sampling function\n",
        "def sample(softmax_predictions, sample_threshold=1.0):\n",
        "    softmax_preds = np.asarray(softmax_predictions).astype('float64')\n",
        "    log_preds = np.log(softmax_preds) / sample_threshold  # Log normalize and divide by threshold\n",
        "    exp_preds = np.exp(log_preds)  # Compute exponents of log normalized terms\n",
        "    norm_preds = exp_preds / np.sum(exp_preds)  # Normalize predictions\n",
        "    prob = np.random.multinomial(1, norm_preds, 1)  # Draw sample from multinomial distribution\n",
        "    return np.argmax(prob)  # Return max value\n",
        "\n",
        "# Custom callback to generate text\n",
        "def on_epoch_end(epoch, _):\n",
        "    global model, model_name\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    start_index = random.randint(0, len(text) - seq_len - 1)  # Random index position to start sample input sequence\n",
        "    end_index = start_index + seq_len  # End of sequence, corresponding to training sequence length\n",
        "    sampling_range = [0.3, 0.5, 0.7, 1.0, 1.2]  # Sampling entropy threshold\n",
        "\n",
        "    for threshold in sampling_range:\n",
        "        print('----- *Sampling Threshold* :', threshold)\n",
        "        generated = ''  # Empty string to collect sequence\n",
        "        sentence = text[start_index: end_index]  # Random input sequence taken from Hamlet\n",
        "        generated += sentence  # Add input sentence to generated\n",
        "        print('Input sequence to generate from : \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)  # Print out buffer instead of waiting till the end\n",
        "\n",
        "        for i in range(400):  # Generate 400 next characters in the sequence\n",
        "            x_pred = np.zeros((1, seq_len, len(characters)), dtype=bool)  # Use `bool` instead of `np.bool`\n",
        "            for n, char in enumerate(sentence):  # For character in sentence\n",
        "                x_pred[0, n, char_indices[char]] = 1  # Change index position for character to 1.\n",
        "            preds = model.predict(x_pred, verbose=0)[0]  # Make prediction on input vector\n",
        "            next_index = sample(preds, threshold)  # Get index position of next character using sample function\n",
        "            next_char = indices_char[next_index]  # Get next character using index\n",
        "            generated += next_char  # Add generated character to sequence\n",
        "            sentence = sentence[1:] + next_char\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "# Function to test multiple models\n",
        "def test_models(list, epochs=10):\n",
        "    global model, model_name\n",
        "    for network in list:\n",
        "        print('Initiating compilation...')\n",
        "        model = network()  # Initialize model\n",
        "        model_name = re.split(' ', str(network))[1]  # Get model name\n",
        "        filepath = \"C:/Users/npurk/Desktop/Ch5RNN/all_models/versions/%s_epoch-{epoch:02d}-loss-{loss:.4f}.h5\" % model_name\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')  # Checkpoint callback object\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam')  # Compile model\n",
        "        print('Compiled:', str(model_name))\n",
        "        network = model.fit(x, y, batch_size=100, epochs=epochs, callbacks=[print_callback, checkpoint])  # Initiate training\n",
        "        model.summary()  # Print model configuration\n",
        "        with open('C:/Users/npurk/Desktop/Ch5RNN/all_models/history/%s.pkl' % model_name, 'wb') as file_pi:\n",
        "            pickle.dump(network.history, file_pi)  # Save model history object for later analysis\n",
        "\n",
        "# Define different RNN models\n",
        "def SimpleRNN_stacked_model():\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(128, input_shape=(seq_len, len(characters)), return_sequences=True))\n",
        "    model.add(SimpleRNN(128))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def GRU_stacked_model():\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape=(seq_len, len(characters)), return_sequences=True))\n",
        "    model.add(GRU(128))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def Bi_directional_GRU():\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=(seq_len, len(characters))))\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def larger_GRU():\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape=(seq_len, len(characters)), dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "    model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "    model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(len(characters), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# List of all defined models\n",
        "all_models = [SimpleRNN_stacked_model, GRU_stacked_model, Bi_directional_GRU, larger_GRU]\n",
        "\n",
        "# Test all models\n",
        "test_models(all_models, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zBh-qoN7h9GS",
        "outputId": "c896f47c-2d21-4e3f-9a25-2fcc646f8e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating compilation...\n",
            "Compiled: SimpleRNN_stacked_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1667/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 2.4069----- Generating text after Epoch: 0\n",
            "----- *Sampling Threshold* : 0.3\n",
            "Input sequence to generate from : \"uick brown fox jumps\"\n",
            "uick brown fox jumpse , whet he shat heare , whet his my lord ham . and whith wathe ham . whe hom . whet me the shat the shat in his wheld whele , the whele , whed thes me hor . as it what die the she father , what whel his whet mues and loue , the ham . it the ke the mathe do the what what me her shat in whe hem . whet it whe dist in so she be the shaue , what she where , whe she sham . where in the his thes and whi----- *Sampling Threshold* : 0.5\n",
            "Input sequence to generate from : \"uick brown fox jumps\"\n",
            "uick brown fox jumpse ham . whet in hat it she , who she shat panth , whet whll ghe the be nowile ao he mad hame hat ham . oue you ' d what whith , and his what it the whe the she loue , this the mant iurgeres . whew of the ppay me fie he sor . whit dond what so whe whes me the sie the kery hor . i whell ham . i whis whe soone , whele ham . whe may , whe whill . whe hers , she foond what me ham . ene whiss seath , an----- *Sampling Threshold* : 0.7\n",
            "Input sequence to generate from : \"uick brown fox jumps\"\n",
            "uick brown fox jumpsteras of mustides , . the deide tho mowist he peagle we tham . hause our lorine ham . fhame , whize hate co showad hall , the no me aroush hourd , thelle the , whe shit the ke , ghat yeg to her wath ' ghile arrous , to the keade she hing hame . autha kes me the daith whel hot not , and bet she the but so hyaue gees los , and pelen whe dowes mam . it he ald whell : gho , but do my dorde , whe vopes----- *Sampling Threshold* : 1.0\n",
            "Input sequence to generate from : \"uick brown fox jumps\"\n",
            "uick brown fox jumps mart . i ' s mamley , nou ' gwasce choulfaine , you seplep infaere pleemina k' gruid gelwtes hiur matto mage a dissss me dorgelwer ' t tres . so che dolke modether . wue lee whes an ' t sime , day of hingigh in geltesne , yhur facth thy a asie thea gots noweathay porme his , she ktamsen thes , me and lagees o him . whem oh ' d haplenas loo . waed you ytid , and for ghes dillaumicheanligiods me la----- *Sampling Threshold* : 1.2\n",
            "Input sequence to generate from : \"uick brown fox jumps\"\n",
            "uick brown fox jumpstrilst i pratteaanitsoutw got - tor ' that mathor bood , weed wlire iocn . ham . ly fall - wrichiss thull ' thourdurslium varkerwies yit sol . fould yow had , tho preyem : whe mas i ' dich shay hoowkende ' ste deiseacd , he . sh kto ith os i soudchaanen wmy hore whel patefwiellh ) farder bey poushen larde fhr kish hyatime ancalqued ? way nact ' recfly erliett ; etremacde , ani wim ; yes myeum no h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 194ms/step - loss: 2.4066\n",
            "Epoch 2/5\n",
            "\u001b[1m1667/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.8394----- Generating text after Epoch: 1\n",
            "----- *Sampling Threshold* : 0.3\n",
            "Input sequence to generate from : \" brown fox jumps ove\"\n",
            " brown fox jumps ove , and the stoue , and the stoue , and shall the good , the stare , the what the stold , and the ond no my lord ? ham . i may the sint his freate , and the she the this , and bo the from my sorne , and brow , and the what i so for his she the stord , and the mar . i will the where , and the mere , where the shall the good , the mall his in the the she the the the shall , and the will , the she the----- *Sampling Threshold* : 0.5\n",
            "Input sequence to generate from : \" brown fox jumps ove\"\n",
            " brown fox jumps ove , reme from exell , bur where the stoun . what i should your ser , and the shaue or for shall the so lise , where it tree , and the eree . for whil . but the king , and so for the ham . i him , the not the drowing the drese , at he mad a beroues , and shall , more , wher dowe the shis resing , and you and the fre , what i she the marther . this a dough , or his bray , the shall a doen . the shou ----- *Sampling Threshold* : 0.7\n",
            "Input sequence to generate from : \" brown fox jumps ove\"\n",
            " brown fox jumps ove your mort age in the king . a mad it whaw reghor . ham . whus like it be the raue and in as distre ? ham . i dye ould dime lise withing , that , i the selfe that record , and shre frame of his i baue and my lord ? a bent righ ; her hamlet ? hor . what he that the come ther the may , the your the reane and the besion the wer shat , i to shall . the will of mand , and the that ? your recesure , my ----- *Sampling Threshold* : 1.0\n",
            "Input sequence to generate from : \" brown fox jumps ove\"\n",
            " brown fox jumps oves theasin i noulches thes you hambher well , the ord vrether hiu where of ble or liue , not greect , i hast ? not i ' like . oh hould , then late . but then in the the se nekent is bearuth ham . who mast you ham . i ke it ih his weat i mores a didstall : yor cine and ho made . what hiu wall thither curngeene , s m rosfer . it eare the frade , thd browing more thus tree : a so a crulrady dota dill ----- *Sampling Threshold* : 1.2\n",
            "Input sequence to generate from : \" brown fox jumps ove\"\n",
            " brown fox jumps ovey adnetgensl . hew o duthet well hamliugh bid , thre , extrengiond lize rwiel seay , vo nrtwer ' d un shentiy , age at defri.se , ti shous doond . cll ouene , wonk& ore , if brouge : hoe . his theo nowhs wi heafe , our . abst ' d brougd . hom . for are ; wheres he blo nthar . vstoese speliour , as whi hissnot amv so be ih hammy , al o king . the bre ; ghat nires him chast or how , wigh this hoe kr"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 200ms/step - loss: 1.8394\n",
            "Epoch 3/5\n",
            "\u001b[1m1667/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.6995----- Generating text after Epoch: 2\n",
            "----- *Sampling Threshold* : 0.3\n",
            "Input sequence to generate from : \"rown fox jumps over \"\n",
            "rown fox jumps over and and the good haue the with and the proment ham . the pray , and the braue the was , the wall the world , and and and the prayes , and the sing the soule , and the could and the saue the dound and in the marke , and you breath , and the mad , and the was are mar . a what it the farth the wall king . and the the with the prace , the mar . the prayer , and the polon . not the see , and indeed , t----- *Sampling Threshold* : 0.5\n",
            "Input sequence to generate from : \"rown fox jumps over \"\n",
            "rown fox jumps over the braue there fares and ham . no , and the good , that he deade ham . and are mest all my lord , and ham . it not the laertes , and them and touch and not loue , and enguer , recersime are my lord , i his , whing as i ' thare , that i ' tis that is soule made and the lord ? may , the king the veries lie . heare of the leed , the death the haue and the douing , and with in the rest what is the da----- *Sampling Threshold* : 0.7\n",
            "Input sequence to generate from : \"rown fox jumps over \"\n",
            "rown fox jumps over , and it nould daue , in the our sale of the good sir laer . what you , and the polch you berie , and and cannaries me the my mich thath doe to my splake , trumes it mect there dad wall goricke goue ankenses and the like my our lourd : be haue king , and as the would in thing his merne , but there are werl on the bornend : and sible , do , i seeath and the truileady ? goods , ald your sellon the m----- *Sampling Threshold* : 1.0\n",
            "Input sequence to generate from : \"rown fox jumps over \"\n",
            "rown fox jumps over may fanlieus ; but swadd i brongermand tim you come ? ox morarce dreetes truth youch loodenconken of screquies bight wightne , a pirgo : queene , the bostaridnyade butzuly canderdacke our selfe of my haues to , the vidistian him . wung deperce sime coplence ophelt ore dett what is attatlentlydurand takes a louertier qf . harlope saioth is true sycricke a thurrainet parry otres , and of his reuelyo----- *Sampling Threshold* : 1.2\n",
            "Input sequence to generate from : \"rown fox jumps over \"\n",
            "rown fox jumps over opherit , yeate ? rovne ? king and my lexteliuet , the doblet : thus bugatioed kadllow mode oury aghianter it) trdotuerer ? rrow , the congy yrath dow fryenerd , by their qeay shald , with a drianntate bate , andert ' markessonbade ,torde it my , your pige auther ? nough : whteues alen that obcemilemay gro ' swinthens ake nirgtes , andownagh polowi: no inrirectyss mush rerift , buld thoues your ab"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 211ms/step - loss: 1.6995\n",
            "Epoch 4/5\n",
            "\u001b[1m1667/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.6252----- Generating text after Epoch: 3\n",
            "----- *Sampling Threshold* : 0.3\n",
            "Input sequence to generate from : \"mps over the lazy do\"\n",
            "mps over the lazy do his concell . exeunt . exe shall so sicke , and the see , and so my lord , i do the stracke a crous , the sir , the see , but the rest not me . and the see . what i do the deene , and let the can . there . a dray you speake the see , and the see . i will not the father , and start , and is the see , and in the conce , there , and for the dend marth , i speake the see , and the play . the slauer ,----- *Sampling Threshold* : 0.5\n",
            "Input sequence to generate from : \"mps over the lazy do\"\n",
            "mps over the lazy do dung of the read me free . exet the does in the crouse , and the me did me , and not we haue not thee , the see , what is the come ind the see in the will a done , and there the such for my lord , and preake hath , and more , and the her with your shall there . i did the peruole , she speere , crance , and the for with a or a surt me as i should me , there the stay your sonnt , and the see , and ----- *Sampling Threshold* : 0.7\n",
            "Input sequence to generate from : \"mps over the lazy do\"\n",
            "mps over the lazy dosa , if reciaration . for there . kin . there the cucken such them in his it is mote , thou like his bore , then will be so . exeull what it his our doder of hath strigation ! it proporicke and sword , and i your they for the indance a with the deer . and tremens in my are it my lord , what a sit the laugh of his speake his conour , and sayer has trement of his all gone row ? qu . let a so not wre----- *Sampling Threshold* : 1.0\n",
            "Input sequence to generate from : \"mps over the lazy do\"\n",
            "mps over the lazy do . i slat . your dotie , for yas returne ) what that hor qu . what i your be manch , sat whon . where not fit . encep , draperon his are . lark . ham . where arti in the profiod there ! harles , with a canke that eare whereage . o fearune it . or the cust he sees ? thee , make not loue fort grrafter your lind . for opheci m ' tion , let looisingon ? alemanke his dungle grrald of his prewor dote , ----- *Sampling Threshold* : 1.2\n",
            "Input sequence to generate from : \"mps over the lazy do\"\n",
            "mps over the lazy do ghan it constew for the grown a guane haighth , for me dink shilly pur . no y is till speare , that cootinch fretsing my roses . ell . miding : dutwer . hose fell butoles louen to it goene plaert is poughts spirgand , yold , gord hoz comed ham . out saune . hay endy , of this more taty this be smoubtt a kisanc? frecian lougq forify , fit , whiloriuipo , to painell igaiigh a munth as ranqiall . ow"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 209ms/step - loss: 1.6251\n",
            "Epoch 5/5\n",
            "\u001b[1m1667/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.5736----- Generating text after Epoch: 4\n",
            "----- *Sampling Threshold* : 0.3\n",
            "Input sequence to generate from : \"jumps over the lazy \"\n",
            "jumps over the lazy to his to so from the doe mar . the pray you the from the consed , the queene , the sould to the pray you so souer like the sould marke to the sould a so pray , and me to so , and the pray you so ham . what should more the soule do the to the see to the parther , the quith the proon the state , and soule the words and to the slaue to soule , and with the was to so sould the play your soule , the r----- *Sampling Threshold* : 0.5\n",
            "Input sequence to generate from : \"jumps over the lazy \"\n",
            "jumps over the lazy , the dranke to see to his are out be a prolie to this makes : so the forth . rost as the well the death to she sir , the warling , and doe a seed more we words of the will the vpit the merough death , and the sould the sone of to so . then and to the wall : what so forfent , and good to the queene , the pray your soule may : the slow rone , the will frall the moother . come to the preenation or t----- *Sampling Threshold* : 0.7\n",
            "Input sequence to generate from : \"jumps over the lazy \"\n",
            "jumps over the lazy out breate , and that say you sweethand haue reere strest fingers haue to the not then i it tagh . but a sinction mary for thy from me a doe a slowince , and ore my lord with i , the draide to crowne in . ham . i are out these by the too he it words , hamlet ham . no , that a brouble so : to his bost the king . my lord , i may a giue : o ' rmands ; there of with all sould he in he illowed : parger----- *Sampling Threshold* : 1.0\n",
            "Input sequence to generate from : \"jumps over the lazy \"\n",
            "jumps over the lazy loue me to his o things desious timall be wight . which ' s dee before dosialeke you qu . as tribbon and meanes manes , i broake to wankeaw stripsing heere speitroung ' d a poyture verken . exeepes to some might : your these pol . now , trob blewes it pirtay so meales of the seage to you guild to to loue , and will that the surphing with likes with iodath daue me be i mor yugrie , and lobutlans , ----- *Sampling Threshold* : 1.2\n",
            "Input sequence to generate from : \"jumps over the lazy \"\n",
            "jumps over the lazy loob yor i ab . how i goo his riiuer of it bedonesse ass crocter franginment heaures , but enseene , quiens pagity : and goo : ile knew lnom to am my not quistian - crilate ? harre vs hamlet with coyfinards wasmeries to mise bod of hassler thus briould dibersding , ham . you playsaid pries . fot haue at heo opaine ard for how my lord i of steeme ham . a stied impord : thirke a make this fame , onc"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1668/1668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 200ms/step - loss: 1.5736\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m22,016\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_3 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m)                  │           \u001b[38;5;34m5,547\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">22,016</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,547</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m181,379\u001b[0m (708.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">181,379</span> (708.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m60,459\u001b[0m (236.17 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,459</span> (236.17 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m120,920\u001b[0m (472.35 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120,920</span> (472.35 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/Users/npurk/Desktop/Ch5RNN/all_models/history/SimpleRNN_stacked_model.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-352403d518e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Test all models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mtest_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-352403d518e1>\u001b[0m in \u001b[0;36mtest_models\u001b[0;34m(list, epochs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initiate training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Print model configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/npurk/Desktop/Ch5RNN/all_models/history/%s.pkl'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save model history object for later analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/npurk/Desktop/Ch5RNN/all_models/history/SimpleRNN_stacked_model.pkl'"
          ]
        }
      ]
    }
  ]
}